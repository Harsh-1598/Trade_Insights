{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1176d650",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde481fb",
   "metadata": {},
   "source": [
    "In this code we will fetch the missing datas about the stock like importing the Sectors and Industry of the stock they are trading, buy and sell time and date. Preprocess the data, perform Data Cleaning, perform Exploratory Data Analysis and at last will do Feature Engineering to find the hidden patterns and trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded14f46",
   "metadata": {},
   "source": [
    "Now we will import the necessary libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b299104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import yfinance as yf\n",
    "from datetime import datetime  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c70596",
   "metadata": {},
   "source": [
    "Now we will load the Dataset to Preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_clean_excel(file_path, start_keywords=['Stock name'], end_keywords=['Unrealised trades', 'Disclaimer']):\n",
    "    # Preview file to find header and footer\n",
    "    preview = pd.read_excel(file_path, header=None)\n",
    "    \n",
    "    # Find where the actual table starts\n",
    "    start_mask = preview.apply(lambda row: row.astype(str).str.contains('|'.join(start_keywords), case=False, na=False)).any(axis=1)\n",
    "    start_index = start_mask[start_mask].index.min()\n",
    "    \n",
    "    # Find where table ends\n",
    "    end_mask = preview.apply(lambda row: row.astype(str).str.contains('|'.join(end_keywords), case=False, na=False)).any(axis=1)\n",
    "    end_index = end_mask[end_mask].index.min() if end_mask.any() else len(preview)\n",
    "    \n",
    "    print(f\"{file_path}\")\n",
    "    print(f\"Data starts at row {start_index}, ends before row {end_index}\")\n",
    "    \n",
    "    # Read only the relevant data rows\n",
    "    df = pd.read_excel(file_path, header=start_index, nrows=end_index - start_index - 1)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = (df.columns\n",
    "                  .astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.replace('\\u00A0', ' ', regex=True)\n",
    "                  .str.replace('\\s+', ' ', regex=True))\n",
    "    \n",
    "    # Drop empty rows/columns\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    print(f\" Clean Data Loaded — Shape: {df.shape}\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Step 1: File Paths\n",
    "pnl_path = \"Stocks_PnL_Report_3788142010_01-10-2020_06-11-2025.xlsx\"\n",
    "order_path = \"Stocks_Order_History_3788142010_01-10-2020_06-11-2025.xlsx\"\n",
    "\n",
    "# Step 2: Load both files dynamically\n",
    "pnl = load_clean_excel(pnl_path)\n",
    "order_history = load_clean_excel(order_path)\n",
    "\n",
    "# Step 3: Verify loaded datasets\n",
    "print(\"PnL Columns:\", pnl.columns.tolist())\n",
    "print(\"Order History Columns:\", order_history.columns.tolist())\n",
    "\n",
    "# Optional — display samples\n",
    "print(\"\\nPnL Preview:\")\n",
    "print(pnl.head(3))\n",
    "\n",
    "print(\"\\nOrder History Preview:\")\n",
    "print(order_history.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b80720",
   "metadata": {},
   "source": [
    "Now let's find some information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()\n",
    "pnl.shape\n",
    "pnl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9967eff",
   "metadata": {},
   "source": [
    "similarly for the Order's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.info()\n",
    "order_history.shape\n",
    "order_history.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d31ca2",
   "metadata": {},
   "source": [
    "Now Let's see how the data looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ab383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.head()\n",
    "pnl.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa968eb0",
   "metadata": {},
   "source": [
    "Similarly for the Order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.head()\n",
    "order_history.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44462e8",
   "metadata": {},
   "source": [
    "Let's check for the missing values and the duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40252de4",
   "metadata": {},
   "source": [
    "Similarly for the order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b91e8",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce09835",
   "metadata": {},
   "source": [
    "Now lets fetch the execution date and time for both the buy and sell orders from the order history to the pnl to do the further preprocesses and find the time-based analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime column exists and is clean\n",
    "order_history.columns = order_history.columns.str.strip().str.replace('\\u00A0', ' ', regex=True)\n",
    "order_history['Execution date and time'] = pd.to_datetime(order_history['Execution date and time'], errors='coerce')\n",
    "\n",
    "# Split date and time safely\n",
    "order_history['Execution Date'] = order_history['Execution date and time'].dt.date\n",
    "order_history['Execution Time'] = order_history['Execution date and time'].dt.time\n",
    "\n",
    "# ---------- Step 3: Clean and convert buy/sell dates safely ----------\n",
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date'], errors='coerce').dt.date\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date'], errors='coerce').dt.date\n",
    "\n",
    "# Drop any rows where dates failed to parse\n",
    "pnl = pnl.dropna(subset=['Buy date', 'Sell date'])\n",
    "\n",
    "# ---------- Step 4: Define market hours helper ----------\n",
    "def is_market_time(dt):\n",
    "    if pd.notna(dt):\n",
    "        time = dt.time()\n",
    "        return (time >= pd.Timestamp('09:15:00').time()) and (time <= pd.Timestamp('15:30:00').time())\n",
    "    return False\n",
    "\n",
    "# ---------- Step 5: Function to get times from order history ----------\n",
    "def get_times_from_order_history(pnl_row, order_df):\n",
    "    stock_name = pnl_row['Stock name']\n",
    "    isin = pnl_row['ISIN']\n",
    "    buy_date = pnl_row['Buy date']\n",
    "    sell_date = pnl_row['Sell date']\n",
    "    \n",
    "    stock_orders = order_df[\n",
    "        (order_df['Stock name'] == stock_name) &\n",
    "        (order_df['ISIN'] == isin)\n",
    "    ].copy()\n",
    "    \n",
    "    # Case 1: Intraday (same date)\n",
    "    if buy_date == sell_date:\n",
    "        buy_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == buy_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'BUY') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').head(1)\n",
    "        \n",
    "        sell_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == sell_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'SELL') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').tail(1)\n",
    "    \n",
    "    # Case 2: Delivery\n",
    "    else:\n",
    "        buy_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == buy_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'BUY') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').head(1)\n",
    "        \n",
    "        sell_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == sell_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'SELL') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').tail(1)\n",
    "    \n",
    "    # Extract times or set defaults\n",
    "    buy_time = buy_order['Execution date and time'].iloc[0].time() if not buy_order.empty else pd.Timestamp('09:15:00').time()\n",
    "    sell_time = sell_order['Execution date and time'].iloc[0].time() if not sell_order.empty else pd.Timestamp('15:30:00').time()\n",
    "    \n",
    "    return buy_time, sell_time\n",
    "\n",
    "# ---------- Step 6: Apply function ----------\n",
    "pnl[['Buy Time', 'Sell Time']] = pnl.apply(\n",
    "    lambda row: pd.Series(get_times_from_order_history(row, order_history)), axis=1\n",
    ")\n",
    "\n",
    "print(\"Buy/Sell times added successfully!\")\n",
    "print(pnl[['Stock name', 'Buy date', 'Buy Time', 'Sell date', 'Sell Time']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af64bfc",
   "metadata": {},
   "source": [
    "Similarly, now let's fetch the sectors and industry for each of the trades to find the sector-based and industry-based trends and patterns in the user trade history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symbol mapping from order history (Symbol to Stock name)\n",
    "symbol_to_name = dict(zip(order_history['Symbol'], order_history['Stock name']))\n",
    "name_to_symbol = {v: k for k, v in symbol_to_name.items()}\n",
    "\n",
    "# Get unique symbols from order history\n",
    "unique_symbols = order_history['Symbol'].unique()\n",
    "\n",
    "# Fetch sector data\n",
    "sector_data = {}\n",
    "for symbol in unique_symbols:\n",
    "    try:\n",
    "        ticker = yf.Ticker(f\"{symbol}.NS\")\n",
    "        info = ticker.info\n",
    "        sector_data[symbol] = (\n",
    "            info.get('sector', 'Unknown'),\n",
    "            info.get('industry', 'Unknown')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {symbol}: {str(e)}\")\n",
    "        sector_data[symbol] = ('Unknown', 'Unknown')\n",
    "\n",
    "# Add to order history\n",
    "order_history['Sector'] = order_history['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[0])\n",
    "order_history['Industry'] = order_history['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[1])\n",
    "\n",
    "# Add to PnL - first extract symbol from stock name\n",
    "pnl['Symbol'] = pnl['Stock name'].map(name_to_symbol)\n",
    "pnl['Sector'] = pnl['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[0])\n",
    "pnl['Industry'] = pnl['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[1])\n",
    "\n",
    "print(\"Sector and Industry data added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28230bea",
   "metadata": {},
   "source": [
    "Since we imported the necessary Timing and Sectorial datas for the trades, now we will be moving towards the Data Cleaning part and afterwards we will apply some EDA techniques to find some hidden patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb9954",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f19c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the information of the datasets again\n",
    "pnl.info()\n",
    "pnl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c049129",
   "metadata": {},
   "source": [
    "Since Remarks have so many missing values, lets fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70902b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Remark'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259194b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Remark'] = pnl['Remark'].fillna('Delivery Trade')\n",
    "pnl['Remark'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7e80c",
   "metadata": {},
   "source": [
    "Since we have duplicate values, let's deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aadb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = pnl.drop_duplicates()\n",
    "pnl.duplicated().sum()\n",
    "pnl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac89c0",
   "metadata": {},
   "source": [
    "Let's handle the missing Sector and Industry names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Sector'].to_list()\n",
    "pnl['Sector'] = pnl['Sector'].replace(['Unknown', ''], 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Industry'].to_list()\n",
    "pnl['Industry'] = pnl['Industry'].replace(['Unknown', ''], 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaab9af",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa733aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()\n",
    "pnl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb6a7f",
   "metadata": {},
   "source": [
    "1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f84a9",
   "metadata": {},
   "source": [
    "Let's do the Univariate Analysis for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numerical_cols = pnl.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Dynamically set rows and columns for subplot grid\n",
    "n_cols = 3\n",
    "n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each numeric column\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(pnl[col].dropna(), kde=True, ax=axes[i], bins=30, color='skyblue')\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontsize=12, weight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "# Remove any unused axes (in case of fewer numeric columns)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff759b",
   "metadata": {},
   "source": [
    "These plots tells us that there are too much skewness in the numerical cols. Also explaining that there are too much correlation between the Numerical cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e046cf7",
   "metadata": {},
   "source": [
    "Now, let's do the Univariate Analysis for the Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a775e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Industry rows appearing less than 4 times\n",
    "uncommon_industry = pnl['Industry'].value_counts() \n",
    "pnl['Industry'] = pnl['Industry'].replace(uncommon_industry[uncommon_industry < 4].index, 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Symbols rows appearing less than 5 times\n",
    "uncommon_Symbols = pnl['Symbol'].value_counts() \n",
    "pnl['Symbol'] = pnl['Symbol'].replace(uncommon_Symbols[uncommon_Symbols < 4].index, 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = pnl[['Remark', 'Symbol', 'Sector', 'Industry']]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 15))\n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "# Plot each categorical column\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    value_counts = pnl[col].value_counts().head(20)  # top 15 categories\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i], palette='pastel')\n",
    "    axes[i].set_title(f\"Count Plot of {col}\", fontsize=12)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "    axes[i].tick_params(axis='x', rotation=60)\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d260f2",
   "metadata": {},
   "source": [
    "First plot tells us that the user has traded mostly intraday then swing trades, also got lucky to get some IPO's allotment and got bonus shares for some of the holdings he had. \n",
    "Second plot tells us that user had traded different stocks while he had traded IndusInd Bank stock the most number of times. Third and Fourth plot tells us that the user had traded the stocks which comes from the different types of sectors and industry but his most traded were Financial Services and Regional Banks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b6d96",
   "metadata": {},
   "source": [
    "Let's plot the graph for the rest of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(19, 10))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Ensure datetime types\n",
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date'], errors='coerce')\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date'], errors='coerce')\n",
    "\n",
    "# Format months as 'MMM-YYYY' (e.g., Jan-2025)\n",
    "pnl['Buy Month'] = pnl['Buy date'].dt.strftime('%b-%Y')\n",
    "pnl['Sell Month'] = pnl['Sell date'].dt.strftime('%b-%Y')\n",
    "\n",
    "# Define 1-hour interval bins for market hours (9:00–15:30)\n",
    "bins = [9, 10, 11, 12, 13, 14, 15, 15.5]\n",
    "labels = ['09:00–10:00', '10:00–11:00', '11:00–12:00', '12:00–13:00', '13:00–14:00', '14:00–15:00', '15:00–15:30']\n",
    "\n",
    "# Extract and bin times\n",
    "pnl['Buy Hour'] = pnl['Buy Time'].apply(lambda x: x.hour + x.minute / 60 if pd.notnull(x) else None)\n",
    "pnl['Sell Hour'] = pnl['Sell Time'].apply(lambda x: x.hour + x.minute / 60 if pd.notnull(x) else None)\n",
    "pnl['Buy Interval'] = pd.cut(pnl['Buy Hour'], bins=bins, labels=labels, right=False)\n",
    "pnl['Sell Interval'] = pd.cut(pnl['Sell Hour'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Buy Time \n",
    "axes[0,0].set_title(\"Distribution of Buy Times (Hourly Intervals)\", fontsize=13)\n",
    "sns.barplot(x=pnl['Buy Interval'].value_counts().reindex(labels).index,\n",
    "            y=pnl['Buy Interval'].value_counts().reindex(labels).values,\n",
    "            ax=axes[0,0], color='steelblue')\n",
    "axes[0,0].set_xlabel(\"Time Interval\")\n",
    "axes[0,0].set_ylabel(\"Number of Buys\")\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Sell Time \n",
    "axes[0,1].set_title(\"Distribution of Sell Times (Hourly Intervals)\", fontsize=13)\n",
    "sns.barplot(x=pnl['Sell Interval'].value_counts().reindex(labels).index,\n",
    "            y=pnl['Sell Interval'].value_counts().reindex(labels).values,\n",
    "            ax=axes[0,1], color='indianred')\n",
    "axes[0,1].set_xlabel(\"Time Interval\")\n",
    "axes[0,1].set_ylabel(\"Number of Sells\")\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Buy Month \n",
    "axes[1, 0].set_title(\"Trades Grouped by Buy Month\", fontsize=13)\n",
    "buy_month_data = pnl['Buy Month'].value_counts().sort_index(key=lambda x: pd.to_datetime(x, format='%b-%Y'))\n",
    "sns.barplot(x=buy_month_data.index, y=buy_month_data.values, ax=axes[1, 0], color='#1f77b4')\n",
    "axes[1, 0].set_xlabel(\"Month-Year\")\n",
    "axes[1, 0].set_ylabel(\"Number of Buys\")\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sell Month \n",
    "axes[1, 1].set_title(\"Trades Grouped by Sell Month\", fontsize=13)\n",
    "sell_month_data = pnl['Sell Month'].value_counts().sort_index(key=lambda x: pd.to_datetime(x, format='%b-%Y'))\n",
    "sns.barplot(x=sell_month_data.index, y=sell_month_data.values, ax=axes[1, 1], color='#d62728')\n",
    "axes[1, 1].set_xlabel(\"Month-Year\")\n",
    "axes[1, 1].set_ylabel(\"Number of Sells\")\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387de08",
   "metadata": {},
   "source": [
    "These Plots tells us that the user had buy the most number of the stocks in the first hour of the day and sell stocks in the last hour of the day while also says that the user was trading from the Nov,21 but were mostly active between the June,22 to Sept,22 and Oct,23 to July,24 telling that he is not focused properly or gets unactive after huge losses and also he had taken some gaps between the months were he was completely unactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd6024",
   "metadata": {},
   "source": [
    "2. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e6ec9",
   "metadata": {},
   "source": [
    "Now, let's do the bivariate analysis. Starting with the plots between the Numerical vs Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for all numerical features\n",
    "sns.pairplot(pnl[numerical_cols], diag_kind='kde', corner=False)\n",
    "plt.suptitle(\"Pairwise Relationships between Numerical Variables\", y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pnl[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Numerical Columns)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a53e22",
   "metadata": {},
   "source": [
    "---------Plots between Numerical vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = pnl.select_dtypes(include=['object']).columns\n",
    "categorical_cols_drop = categorical_cols.drop(['ISIN', 'Stock name'])\n",
    "target = 'Realised P&L'\n",
    "\n",
    "if target not in pnl.columns:\n",
    "    print(\"Target column 'Realised P&L' not found — skipping plots.\")\n",
    "else:\n",
    "    for col in categorical_cols_drop:\n",
    "        n_unique = pnl[col].nunique()\n",
    "        \n",
    "        if col in ['Buy Time', 'Sell Time']:\n",
    "            if col == 'Buy Time':\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "                \n",
    "                for idx, time_col in enumerate(['Buy Time', 'Sell Time']):\n",
    "                    pnl_time = pnl.copy()\n",
    "                    pnl_time[f'{time_col}_Hour'] = pd.to_datetime(pnl_time[time_col], \n",
    "                                                                   format='%H:%M:%S', errors='coerce').dt.hour\n",
    "                    \n",
    "                    hourly_data = pnl_time.groupby(f'{time_col}_Hour')[target].agg(['mean', 'count']).reset_index()\n",
    "                    hourly_data = hourly_data.sort_values(f'{time_col}_Hour')\n",
    "                    \n",
    "                    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in hourly_data['mean']]\n",
    "                    \n",
    "                    axes[idx].bar(hourly_data[f'{time_col}_Hour'], hourly_data['mean'], \n",
    "                                 color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "                    axes[idx].set_title(f\"Average {target} by {time_col} (Hourly)\", \n",
    "                                       fontsize=15, fontweight='bold', pad=15)\n",
    "                    axes[idx].set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "                    axes[idx].set_ylabel(f'Average {target}', fontsize=12, fontweight='bold')\n",
    "                    axes[idx].axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "                    axes[idx].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "                    axes[idx].spines['top'].set_visible(False)\n",
    "                    axes[idx].spines['right'].set_visible(False)\n",
    "                    axes[idx].set_xticks(hourly_data[f'{time_col}_Hour'])\n",
    "                    \n",
    "                    for i, row in hourly_data.iterrows():\n",
    "                        offset = 20 if row['mean'] > 0 else -20\n",
    "                        axes[idx].text(row[f'{time_col}_Hour'], row['mean'] + offset, \n",
    "                                      f\"n={int(row['count'])}\", \n",
    "                                      ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "                                      bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                               edgecolor='gray', alpha=0.8))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        elif col == 'Symbol':\n",
    "            fig, ax = plt.subplots(figsize=(16, 9))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.barh(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                          edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_yticks(range(len(stats)))\n",
    "            ax.set_yticklabels(stats[col], fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 20 if row['mean'] > 0 else -20\n",
    "                ax.text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                       va='center', ha='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        elif col in ['Sector', 'Industry']:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(24, 9))\n",
    "            \n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'sum', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors_mean = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['mean']]\n",
    "            bars0 = axes[0].barh(range(len(stats)), stats['mean'], color=colors_mean, \n",
    "                                alpha=0.8, edgecolor='white', linewidth=2)\n",
    "            \n",
    "            axes[0].set_yticks(range(len(stats)))\n",
    "            axes[0].set_yticklabels(stats[col], fontsize=11, fontweight='bold', rotation = 30)\n",
    "            axes[0].set_title(f'Average {target} by {col}', fontsize=14, fontweight='bold', pad=15)\n",
    "            axes[0].set_xlabel(f'Average {target}', fontsize=12, fontweight='bold')\n",
    "            axes[0].axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            axes[0].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            axes[0].spines['top'].set_visible(False)\n",
    "            axes[0].spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 50 if row['mean'] > 0 else -50\n",
    "                axes[0].text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                           va='center', ha='center', fontsize=9, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                    edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            colors_total = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['sum']]\n",
    "            bars1 = axes[1].barh(range(len(stats)), stats['sum'], color=colors_total, \n",
    "                                alpha=0.8, edgecolor='white', linewidth=2)\n",
    "            \n",
    "            axes[1].set_yticks(range(len(stats)))\n",
    "            axes[1].set_yticklabels(stats[col], fontsize=11, fontweight='bold', rotation = 30)\n",
    "            axes[1].set_title(f'Total {target} by {col}', fontsize=14, fontweight='bold', pad=15)\n",
    "            axes[1].set_xlabel(f'Total {target}', fontsize=12, fontweight='bold')\n",
    "            axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            axes[1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            axes[1].spines['top'].set_visible(False)\n",
    "            axes[1].spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 400 if row['sum'] > 0 else -400\n",
    "                axes[1].text(row['sum'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                           va='center', ha='center', fontsize=9, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                    edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        elif col == 'Remark':\n",
    "            fig, ax = plt.subplots(figsize=(8, 7))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=False)\n",
    "            \n",
    "            colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.bar(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                         edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(range(len(stats)))\n",
    "            ax.set_xticklabels(stats[col], fontsize=11, fontweight='bold', rotation=45, ha='right')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_ylabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 50 if row['mean'] > 0 else -50\n",
    "                ax.text(i, row['mean'] + offset, f\"n={int(row['count'])}\", \n",
    "                       ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(16, 7))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.barh(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                          edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_yticks(range(len(stats)))\n",
    "            ax.set_yticklabels(stats[col], fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 150 if row['mean'] > 0 else -150\n",
    "                ax.text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                       va='center', ha='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4209c",
   "metadata": {},
   "source": [
    "-----------Plots between Categorical vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_pairs = [\n",
    "    ('Buy Month', 'Sell Month'),\n",
    "    ('Buy Month', 'Sector'),\n",
    "    ('Sell Month', 'Sector'),\n",
    "    ('Remark', 'Sector'),\n",
    "    ('Remark', 'Industry'),\n",
    "    ('Buy Month', 'Remark'),\n",
    "    ('Sell Month', 'Remark')\n",
    "]\n",
    "\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "for col1, col2 in important_pairs:\n",
    "    if col1 in pnl.columns and col2 in pnl.columns:\n",
    "        if pnl[col1].nunique() <= 20 and pnl[col2].nunique() <= 20:\n",
    "            \n",
    "            if col1 in ['Buy Month', 'Sell Month']:\n",
    "                pnl_temp = pnl[pnl[col1].isin(month_order)].copy()\n",
    "                pnl_temp[col1] = pd.Categorical(pnl_temp[col1], categories=month_order, ordered=True)\n",
    "            else:\n",
    "                pnl_temp = pnl.copy()\n",
    "            \n",
    "            if col2 in ['Buy Month', 'Sell Month']:\n",
    "                pnl_temp = pnl_temp[pnl_temp[col2].isin(month_order)].copy()\n",
    "                pnl_temp[col2] = pd.Categorical(pnl_temp[col2], categories=month_order, ordered=True)\n",
    "            \n",
    "            cross_tab = pd.crosstab(pnl_temp[col1], pnl_temp[col2])\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 9))\n",
    "            \n",
    "            sns.heatmap(cross_tab, cmap='YlOrRd', annot=True, fmt='d', \n",
    "                       linewidths=2, linecolor='white',\n",
    "                       cbar_kws={'label': 'Trade Count', 'shrink': 0.8},\n",
    "                       annot_kws={'fontsize': 11, 'fontweight': 'bold'},\n",
    "                       ax=ax)\n",
    "            \n",
    "            ax.set_title(f\"Relationship between {col1} and {col2}\", \n",
    "                        fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(col2, fontsize=13, fontweight='bold', labelpad=10)\n",
    "            ax.set_ylabel(col1, fontsize=13, fontweight='bold', labelpad=10)\n",
    "            \n",
    "            ax.tick_params(axis='x', labelsize=11, rotation=45)\n",
    "            ax.tick_params(axis='y', labelsize=11, rotation=0)\n",
    "            \n",
    "            plt.setp(ax.get_xticklabels(), ha='right', rotation_mode='anchor')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a583fe",
   "metadata": {},
   "source": [
    "--------- These plots..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed598c",
   "metadata": {},
   "source": [
    "we can also use the Pandas Profiller to apply the EDA to the User Trade History dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(pnl, title=\"User Trade History Analysis\")\n",
    "profile.to_file(\"User Trade History Analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc50dc",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78217225",
   "metadata": {},
   "source": [
    "Since the realised profit and loss is not calculating the charges, taxes and brokerages paid by the user to the Broker and Exchange so we will be calculating them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets Calculate the Charges, Taxes and Brokerages for each of the trades.\n",
    "# # Calculating the Brokerage\n",
    "# pnl['Brokerage'] = (pnl['Buy value'] + pnl['Sell value']) * 0.0005\n",
    "# pnl['Brokerage'] = pnl['Brokerage'].apply(lambda x: min(x, 20)) *(1.18)\n",
    "\n",
    "# # Calculating the Exchange Charges, SEBI Charges and Investor Protection Fund\n",
    "# pnl['Exchange Charges'] = (pnl['Buy value'] + pnl['Sell value']) * 0.0000297 *(1.18)\n",
    "# pnl['SEBI Charges'] = (pnl['Buy value'] + pnl['Sell value']) * 0.000001 *(1.18)\n",
    "# pnl['Investor Protection Fund'] = (pnl['Buy value'] + pnl['Sell value']) * 0.000001 *(1.18)\n",
    "\n",
    "# # Initialize columns\n",
    "# pnl['STT'] = 0\n",
    "# pnl['Stamp Duty'] = 0\n",
    "\n",
    "# # For Intraday\n",
    "# pnl.loc[pnl['Remark'] == 'Intraday', 'STT'] = pnl['Sell value'] * 0.00025  *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'Intraday', 'Stamp Duty'] = pnl['Buy value'] * 0.00003 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'Intraday', 'Demat Charges'] = 0 *(1.18)\n",
    "\n",
    "# # For Delivery (or others)\n",
    "# pnl.loc[pnl['Remark'] != 'Intraday', 'STT'] = (pnl['Sell value'] + pnl['Buy value']) * 0.001 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] != 'Intraday', 'Stamp Duty'] = pnl['Buy value'] * 0.00015 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] != 'Intraday', 'Demat Charges'] = 13.5 *(1.18)\n",
    "\n",
    "# # For IPO\n",
    "# pnl.loc[pnl['Remark'] == 'New shares credit from IPO', 'STT'] = (pnl['Sell value']) * 0.001 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'New shares credit from IPO', 'Stamp Duty'] = 0 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'New shares credit from IPO', 'Exchange Charges'] = (pnl['Sell value']) * 0.0000297 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'New shares credit from IPO', 'SEBI Charges'] = (pnl['Sell value']) * 0.000001 *(1.18)\n",
    "# pnl.loc[pnl['Remark'] == 'New shares credit from IPO', 'Investor Protection Fund'] = (pnl['Sell value']) * 0.000001 *(1.18)\n",
    "\n",
    "# # Calculating the Total Charges including GST\n",
    "# pnl['Total Charges'] = (pnl['Brokerage'] + pnl['STT'] + pnl['Exchange Charges'] + pnl['SEBI Charges'] + pnl['Investor Protection Fund'] + pnl['Stamp Duty'] + pnl['Demat Charges'])\n",
    "\n",
    "# # Since we have calculated the charges, taxes and brokerages for each of the trades, we can now drop the unnecessary columns.\n",
    "# charges_to_drop = ['STT', 'Exchange Charges', 'SEBI Charges', 'Investor Protection Fund', 'Stamp Duty', 'Demat Charges']\n",
    "# pnl = pnl.drop(columns=charges_to_drop)\n",
    "\n",
    "# # Adding a new column for Total charges in the PnL dataset\n",
    "# pnl['Total Charges'] = pnl['Total Charges'].round(4)\n",
    "# pnl.to_excel('Stocks_PnL_3788142010_01-10-2021_11-10-2025_report.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c730036",
   "metadata": {},
   "source": [
    "Now, let's find the Gross and Net pnl and pnl % to better understand the profits and losses. This will give the idea of overall profits user generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aaf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the Gross and Net pnl and pnl %\n",
    "# pnl['Gross Realised P&L'] = pnl['Sell value'] - pnl['Buy value']\n",
    "# pnl['Gross Realised PnL %'] = (pnl['Gross Realised P&L'] / pnl['Buy value']) * 100\n",
    "# pnl['Net Realised P&L'] = pnl['Gross Realised P&L'] - pnl['Total Charges']\n",
    "# pnl['Net Realised PnL %'] = (pnl['Net Realised P&L'] / pnl['Buy value']) * 100\n",
    "# pnl = pnl.drop(columns= 'Realised P&L')\n",
    "# pnl.to_excel('Stocks_PnL_3788142010_01-10-2021_11-10-2025_report.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ad2fd",
   "metadata": {},
   "source": [
    "Now, let's find the holding period to understand the user's preference and find the most profitable number of days for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert dates to datetime\n",
    "# pnl['Buy date'] = pd.to_datetime(pnl['Buy date'], dayfirst=True)\n",
    "# pnl['Sell date'] = pd.to_datetime(pnl['Sell date'], dayfirst=True)\n",
    "\n",
    "# # 1. Calculate Holding Period (in days)\n",
    "# pnl['Holding Period (Days)'] = (pnl['Sell date'] - pnl['Buy date']).dt.days\n",
    "# pnl.to_excel('Stocks_PnL_3788142010_01-10-2021_11-10-2025_report.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
