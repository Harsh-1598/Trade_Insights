{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1176d650",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde481fb",
   "metadata": {},
   "source": [
    "In this code we will fetch the missing datas about the stock like importing the Sectors and Industry of the stock they are trading, buy and sell time and date. Preprocess the data, perform Data Cleaning, perform Exploratory Data Analysis and at last will do Feature Engineering to find the hidden patterns and trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded14f46",
   "metadata": {},
   "source": [
    "Now we will import the necessary libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b299104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import yfinance as yf\n",
    "from datetime import datetime  \n",
    "from datetime import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c70596",
   "metadata": {},
   "source": [
    "Now we will load the Dataset to Preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_clean_excel(file_path, start_keywords=['Stock name'], end_keywords=['Unrealised trades', 'Disclaimer']):\n",
    "    # Preview file to find header and footer\n",
    "    preview = pd.read_excel(file_path, header=None)\n",
    "    \n",
    "    # Find where the actual table starts\n",
    "    start_mask = preview.apply(lambda row: row.astype(str).str.contains('|'.join(start_keywords), case=False, na=False)).any(axis=1)\n",
    "    start_index = start_mask[start_mask].index.min()\n",
    "    \n",
    "    # Find where table ends\n",
    "    end_mask = preview.apply(lambda row: row.astype(str).str.contains('|'.join(end_keywords), case=False, na=False)).any(axis=1)\n",
    "    end_index = end_mask[end_mask].index.min() if end_mask.any() else len(preview)\n",
    "    \n",
    "    print(f\"{file_path}\")\n",
    "    print(f\"Data starts at row {start_index}, ends before row {end_index}\")\n",
    "    \n",
    "    # Read only the relevant data rows\n",
    "    df = pd.read_excel(file_path, header=start_index, nrows=end_index - start_index - 1)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = (df.columns\n",
    "                  .astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.replace('\\u00A0', ' ', regex=True)\n",
    "                  .str.replace('\\s+', ' ', regex=True))\n",
    "    \n",
    "    # Drop empty rows/columns\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    print(f\" Clean Data Loaded — Shape: {df.shape}\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Step 1: File Paths\n",
    "# ---- Harsh ----\n",
    "pnl_path = \"Stocks_PnL_Report_3788142010_01-10-2020_06-11-2025.xlsx\"\n",
    "order_path = \"Stocks_Order_History_3788142010_01-10-2020_06-11-2025.xlsx\"\n",
    "\n",
    "# ---- Rudra ----\n",
    "# pnl_path = \"Stocks_PnL_4961823605_01-04-2023_08-11-2025_report.xlsx\" \n",
    "# order_path = \"Stocks_Order_History_4961823605_2023-04-01_2025-11-08_1762690161916.xlsx\"\n",
    "\n",
    "# Step 2: Load both files dynamically\n",
    "pnl = load_clean_excel(pnl_path)\n",
    "order_history = load_clean_excel(order_path)\n",
    "\n",
    "# Step 3: Verify loaded datasets\n",
    "print(\"PnL Columns:\", pnl.columns.tolist())\n",
    "print(\"Order History Columns:\", order_history.columns.tolist())\n",
    "\n",
    "# Optional — display samples\n",
    "print(\"\\nPnL Preview:\")\n",
    "print(pnl.head(3))\n",
    "\n",
    "print(\"\\nOrder History Preview:\")\n",
    "print(order_history.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b80720",
   "metadata": {},
   "source": [
    "Now let's find some information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()\n",
    "pnl.shape\n",
    "pnl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9967eff",
   "metadata": {},
   "source": [
    "similarly for the Order's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.info()\n",
    "order_history.shape\n",
    "order_history.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d31ca2",
   "metadata": {},
   "source": [
    "Now Let's see how the data looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ab383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.head()\n",
    "pnl.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa968eb0",
   "metadata": {},
   "source": [
    "Similarly for the Order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.head()\n",
    "order_history.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44462e8",
   "metadata": {},
   "source": [
    "Let's check for the missing values and the duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40252de4",
   "metadata": {},
   "source": [
    "Similarly for the order history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_history.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b91e8",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce09835",
   "metadata": {},
   "source": [
    "Now lets fetch the execution date and time for both the buy and sell orders from the order history to the pnl to do the further preprocesses and find the time-based analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime column exists and is clean\n",
    "order_history.columns = order_history.columns.str.strip().str.replace('\\u00A0', ' ', regex=True)\n",
    "order_history['Execution date and time'] = pd.to_datetime(order_history['Execution date and time'], errors='coerce')\n",
    "\n",
    "# Split date and time safely\n",
    "order_history['Execution Date'] = order_history['Execution date and time'].dt.date\n",
    "order_history['Execution Time'] = order_history['Execution date and time'].dt.time\n",
    "\n",
    "# ---------- Step 3: Clean and convert buy/sell dates safely ----------\n",
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date'], errors='coerce').dt.date\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date'], errors='coerce').dt.date\n",
    "\n",
    "# Drop any rows where dates failed to parse\n",
    "pnl = pnl.dropna(subset=['Buy date', 'Sell date'])\n",
    "\n",
    "# ---------- Step 4: Define market hours helper ----------\n",
    "def is_market_time(dt):\n",
    "    if pd.notna(dt):\n",
    "        time = dt.time()\n",
    "        return (time >= pd.Timestamp('09:15:00').time()) and (time <= pd.Timestamp('15:30:00').time())\n",
    "    return False\n",
    "\n",
    "# ---------- Step 5: Function to get times from order history ----------\n",
    "def get_times_from_order_history(pnl_row, order_df):\n",
    "    stock_name = pnl_row['Stock name']\n",
    "    isin = pnl_row['ISIN']\n",
    "    buy_date = pnl_row['Buy date']\n",
    "    sell_date = pnl_row['Sell date']\n",
    "    \n",
    "    stock_orders = order_df[\n",
    "        (order_df['Stock name'] == stock_name) &\n",
    "        (order_df['ISIN'] == isin)\n",
    "    ].copy()\n",
    "    \n",
    "    # Case 1: Intraday (same date)\n",
    "    if buy_date == sell_date:\n",
    "        buy_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == buy_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'BUY') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').head(1)\n",
    "        \n",
    "        sell_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == sell_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'SELL') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').tail(1)\n",
    "    \n",
    "    # Case 2: Delivery\n",
    "    else:\n",
    "        buy_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == buy_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'BUY') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').head(1)\n",
    "        \n",
    "        sell_order = stock_orders[\n",
    "            (stock_orders['Execution date and time'].dt.date == sell_date) &\n",
    "            (stock_orders['Type'].str.upper() == 'SELL') &\n",
    "            (stock_orders['Execution date and time'].apply(is_market_time))\n",
    "        ].sort_values('Execution date and time').tail(1)\n",
    "    \n",
    "    # Extract times or set defaults\n",
    "    buy_time = buy_order['Execution date and time'].iloc[0].time() if not buy_order.empty else pd.Timestamp('09:15:00').time()\n",
    "    sell_time = sell_order['Execution date and time'].iloc[0].time() if not sell_order.empty else pd.Timestamp('15:30:00').time()\n",
    "    \n",
    "    return buy_time, sell_time\n",
    "\n",
    "# ---------- Step 6: Apply function ----------\n",
    "pnl[['Buy Time', 'Sell Time']] = pnl.apply(\n",
    "    lambda row: pd.Series(get_times_from_order_history(row, order_history)), axis=1\n",
    ")\n",
    "\n",
    "print(\"Buy/Sell times added successfully!\")\n",
    "print(pnl[['Stock name', 'Buy date', 'Buy Time', 'Sell date', 'Sell Time']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af64bfc",
   "metadata": {},
   "source": [
    "Similarly, now let's fetch the sectors and industry for each of the trades to find the sector-based and industry-based trends and patterns in the user trade history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symbol mapping from order history (Symbol to Stock name)\n",
    "symbol_to_name = dict(zip(order_history['Symbol'], order_history['Stock name']))\n",
    "name_to_symbol = {v: k for k, v in symbol_to_name.items()}\n",
    "\n",
    "# Get unique symbols from order history\n",
    "unique_symbols = order_history['Symbol'].unique()\n",
    "\n",
    "# Fetch sector data\n",
    "sector_data = {}\n",
    "for symbol in unique_symbols:\n",
    "    try:\n",
    "        ticker = yf.Ticker(f\"{symbol}.NS\")\n",
    "        info = ticker.info\n",
    "        sector_data[symbol] = (\n",
    "            info.get('sector', 'Unknown'),\n",
    "            info.get('industry', 'Unknown')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {symbol}: {str(e)}\")\n",
    "        sector_data[symbol] = ('Unknown', 'Unknown')\n",
    "\n",
    "# Add to order history\n",
    "order_history['Sector'] = order_history['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[0])\n",
    "order_history['Industry'] = order_history['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[1])\n",
    "\n",
    "# Add to PnL - first extract symbol from stock name\n",
    "pnl['Symbol'] = pnl['Stock name'].map(name_to_symbol)\n",
    "pnl['Sector'] = pnl['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[0])\n",
    "pnl['Industry'] = pnl['Symbol'].map(lambda x: sector_data.get(x, ('Unknown', 'Unknown'))[1])\n",
    "\n",
    "print(\"Sector and Industry data added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28230bea",
   "metadata": {},
   "source": [
    "Since we imported the necessary Timing and Sectorial datas for the trades, now we will be moving towards the Data Cleaning part and afterwards we will apply some EDA techniques to find some hidden patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb9954",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f19c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the information of the datasets again\n",
    "pnl.info()\n",
    "pnl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c049129",
   "metadata": {},
   "source": [
    "Since Remarks have so many missing values, lets fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70902b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Remark'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259194b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Remark'] = pnl['Remark'].fillna('Delivery Trade')\n",
    "pnl['Remark'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7e80c",
   "metadata": {},
   "source": [
    "Since we have duplicate values, let's deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aadb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = pnl.drop_duplicates()\n",
    "pnl.duplicated().sum()\n",
    "pnl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac89c0",
   "metadata": {},
   "source": [
    "Let's handle the missing Sector and Industry names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Sector'].to_list()\n",
    "pnl['Sector'] = pnl['Sector'].replace(['Unknown', ''], 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Industry'].to_list()\n",
    "pnl['Industry'] = pnl['Industry'].replace(['Unknown', ''], 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaab9af",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa733aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()\n",
    "pnl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb019997",
   "metadata": {},
   "source": [
    "Before Proceedings we will find the Buy Hour, Buy Month, Buy Year, and similarly will find for the Selling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date'])\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date'])\n",
    "\n",
    "pnl['Buy Year'] = pnl['Buy date'].dt.year.astype('int64')\n",
    "pnl['Buy Month'] = pnl['Buy date'].dt.strftime('%b').astype('object')\n",
    "pnl['Buy Day'] = pnl['Buy date'].dt.day.astype('int64')\n",
    "pnl['Buy Day of Week'] = pnl['Buy date'].dt.strftime('%A').astype('object')\n",
    "\n",
    "pnl['Sell Year'] = pnl['Sell date'].dt.year.astype('int64')\n",
    "pnl['Sell Month'] = pnl['Sell date'].dt.strftime('%b').astype('object')\n",
    "pnl['Sell Day'] = pnl['Sell date'].dt.day.astype('int64')\n",
    "pnl['Sell Day of Week'] = pnl['Sell date'].dt.strftime('%A').astype('object')\n",
    "\n",
    "if 'Buy Time' in pnl.columns:\n",
    "    pnl['Buy Time'] = pd.to_datetime(pnl['Buy Time'], format='%H:%M:%S', errors='coerce')\n",
    "    pnl['Buy Hour'] = pnl['Buy Time'].dt.hour.astype('int64')\n",
    "    \n",
    "    pnl['Buy Interval'] = pnl['Buy Time'].apply(lambda x: \n",
    "        f\"{x.hour:02d}:{(x.minute // 30) * 30:02d}-{x.hour if (x.minute // 30) * 30 + 30 < 60 else x.hour + 1:02d}:{(x.minute // 30) * 30 + 30 if (x.minute // 30) * 30 + 30 < 60 else 0:02d}\"\n",
    "        if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    pnl['Buy Time'] = pnl['Buy Time'].dt.time\n",
    "\n",
    "if 'Sell Time' in pnl.columns:\n",
    "    pnl['Sell Time'] = pd.to_datetime(pnl['Sell Time'], format='%H:%M:%S', errors='coerce')\n",
    "    pnl['Sell Hour'] = pnl['Sell Time'].dt.hour.astype('int64')\n",
    "    \n",
    "    pnl['Sell Interval'] = pnl['Sell Time'].apply(lambda x: \n",
    "        f\"{x.hour:02d}:{(x.minute // 30) * 30:02d}-{x.hour if (x.minute // 30) * 30 + 30 < 60 else x.hour + 1:02d}:{(x.minute // 30) * 30 + 30 if (x.minute // 30) * 30 + 30 < 60 else 0:02d}\"\n",
    "        if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    pnl['Sell Time'] = pnl['Sell Time'].dt.time\n",
    "\n",
    "if 'Buy Interval' in pnl.columns:\n",
    "    pnl['Buy Interval'] = pnl['Buy Interval'].replace('15:30-16:00', '15:00-15:30')\n",
    "    pnl['Buy Interval'] = pnl['Buy Interval'].fillna('09:00-10:00')\n",
    "    pnl['Buy Interval'] = pnl['Buy Interval'].astype('object')\n",
    "\n",
    "if 'Sell Interval' in pnl.columns:\n",
    "    pnl['Sell Interval'] = pnl['Sell Interval'].replace('15:30-16:00', '15:00-15:30')\n",
    "    pnl['Sell Interval'] = pnl['Sell Interval'].fillna('15:00-15:30')\n",
    "    pnl['Sell Interval'] = pnl['Sell Interval'].astype('object')\n",
    "\n",
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date']).dt.date\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date']).dt.date\n",
    "\n",
    "# Convert Buy Time and Sell Time to numerical format while preserving all information\n",
    "# Convert to minutes from market open (09:00:00)\n",
    "pnl['Buy Time Minutes'] = pd.to_datetime(pnl['Buy Time'], format='%H:%M:%S').apply(\n",
    "    lambda x: (x.hour * 60 + x.minute) if pd.notna(x) else None\n",
    ").astype('float64')\n",
    "\n",
    "pnl['Sell Time Minutes'] = pd.to_datetime(pnl['Sell Time'], format='%H:%M:%S').apply(\n",
    "    lambda x: (x.hour * 60 + x.minute) if pd.notna(x) else None\n",
    ").astype('float64')\n",
    "\n",
    "# Convert Buy Hour and Sell Hour to decimal format (preserves minutes)\n",
    "pnl['Buy Hour Decimal'] = pd.to_datetime(pnl['Buy Time'], format='%H:%M:%S').apply(\n",
    "    lambda x: (x.hour + x.minute/60) if pd.notna(x) else None\n",
    ").astype('float64')\n",
    "\n",
    "pnl['Sell Hour Decimal'] = pd.to_datetime(pnl['Sell Time'], format='%H:%M:%S').apply(\n",
    "    lambda x: (x.hour + x.minute/60) if pd.notna(x) else None\n",
    ").astype('float64')\n",
    "\n",
    "# Verify\n",
    "print(pnl[['Buy Time', 'Buy Hour', 'Buy Hour Decimal', 'Buy Interval']].head(10))\n",
    "\n",
    "# Verify conversion\n",
    "print(\"Buy Time format conversions:\")\n",
    "print(pnl[['Buy Time', 'Buy Time Minutes']].head())\n",
    "print(\"\\nSell Time format conversions:\")\n",
    "print(pnl[['Sell Time', 'Sell Time Minutes']].head())\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMISSING VALUES:\")\n",
    "print(\"-\"*80)\n",
    "missing_data = pnl.isnull().sum()\n",
    "print(missing_data[missing_data > 0] if missing_data.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUPLICATE ROWS:\")\n",
    "print(\"-\"*80)\n",
    "duplicates = pnl.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate row indices:\")\n",
    "    print(pnl[pnl.duplicated(keep=False)].index.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPES:\")\n",
    "print(\"-\"*80)\n",
    "print(pnl.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb6a7f",
   "metadata": {},
   "source": [
    "1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f84a9",
   "metadata": {},
   "source": [
    "Let's do the Univariate Analysis for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numerical_cols = pnl.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Dynamically set rows and columns for subplot grid\n",
    "n_cols = 3\n",
    "n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each numeric column\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(pnl[col].dropna(), kde=True, ax=axes[i], bins=30, color='skyblue')\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontsize=12, weight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "# Remove any unused axes (in case of fewer numeric columns)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff759b",
   "metadata": {},
   "source": [
    "These plots tells us that there are too much skewness in the numerical cols. Also explaining that there are too much correlation between the Numerical cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e046cf7",
   "metadata": {},
   "source": [
    "Now, let's do the Univariate Analysis for the Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a775e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Industry rows appearing less than 4 times\n",
    "uncommon_industry = pnl['Industry'].value_counts() \n",
    "pnl['Industry'] = pnl['Industry'].replace(uncommon_industry[uncommon_industry < 4].index, 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Symbols rows appearing less than 5 times\n",
    "uncommon_Symbols = pnl['Symbol'].value_counts() \n",
    "pnl['Symbol'] = pnl['Symbol'].replace(uncommon_Symbols[uncommon_Symbols < 4].index, 'Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = pnl[['Remark', 'Symbol', 'Sector', 'Industry']]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 15))\n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "# Plot each categorical column\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    value_counts = pnl[col].value_counts().head(20)  # top 15 categories\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i], palette='pastel')\n",
    "    axes[i].set_title(f\"Count Plot of {col}\", fontsize=12)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "    axes[i].tick_params(axis='x', rotation=60)\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d260f2",
   "metadata": {},
   "source": [
    "First plot tells us that the user has traded mostly intraday then swing trades, also got lucky to get some IPO's allotment and got bonus shares for some of the holdings he had. \n",
    "Second plot tells us that user had traded different stocks while he had traded IndusInd Bank stock the most number of times. Third and Fourth plot tells us that the user had traded the stocks which comes from the different types of sectors and industry but his most traded were Financial Services and Regional Banks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b6d96",
   "metadata": {},
   "source": [
    "Let's plot the graph for the rest of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(19, 5))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Buy Time - ordered by time interval\n",
    "axes[0].set_title(\"Distribution of Buy Times (30-Mins Intervals)\", fontsize=13)\n",
    "buy_time_data = pnl['Buy Interval'].value_counts().sort_index()\n",
    "sns.barplot(x=buy_time_data.index, y=buy_time_data.values, ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel(\"Time Interval\")\n",
    "axes[0].set_ylabel(\"Number of Buys\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Sell Time - ordered by time interval\n",
    "axes[1].set_title(\"Distribution of Sell Times (30-Mins Intervals)\", fontsize=13)\n",
    "sell_time_data = pnl['Sell Interval'].value_counts().sort_index()\n",
    "sns.barplot(x=sell_time_data.index, y=sell_time_data.values, ax=axes[1], color='indianred')\n",
    "axes[1].set_xlabel(\"Time Interval\")\n",
    "axes[1].set_ylabel(\"Number of Sells\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5973e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(19, 10))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Buy Month-Year trades with all months filled\n",
    "axes[0].set_title(\"Trades Grouped by Buy Month-Year\", fontsize=13)\n",
    "pnl['Buy Month-Year'] = pnl['Buy Month'] + '-' + pnl['Buy Year'].astype(str)\n",
    "min_date = pd.to_datetime(pnl['Buy Month-Year'], format='%b-%Y').min()\n",
    "max_date = pd.to_datetime(pnl['Buy Month-Year'], format='%b-%Y').max()\n",
    "all_months_buy = pd.date_range(start=min_date, end=max_date, freq='MS').strftime('%b-%Y')\n",
    "buy_month_year_data = pnl['Buy Month-Year'].value_counts().reindex(all_months_buy, fill_value=0)\n",
    "sns.barplot(x=buy_month_year_data.index, y=buy_month_year_data.values, ax=axes[0], color='#1f77b4')\n",
    "axes[0].set_xlabel(\"Month-Year\")\n",
    "axes[0].set_ylabel(\"Number of Buys\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sell Month-Year trades with all months filled\n",
    "axes[1].set_title(\"Trades Grouped by Sell Month-Year\", fontsize=13)\n",
    "pnl['Sell Month-Year'] = pnl['Sell Month'] + '-' + pnl['Sell Year'].astype(str)\n",
    "min_date = pd.to_datetime(pnl['Sell Month-Year'], format='%b-%Y').min()\n",
    "max_date = pd.to_datetime(pnl['Sell Month-Year'], format='%b-%Y').max()\n",
    "all_months_sell = pd.date_range(start=min_date, end=max_date, freq='MS').strftime('%b-%Y')\n",
    "sell_month_year_data = pnl['Sell Month-Year'].value_counts().reindex(all_months_sell, fill_value=0)\n",
    "sns.barplot(x=sell_month_year_data.index, y=sell_month_year_data.values, ax=axes[1], color='#d62728')\n",
    "axes[1].set_xlabel(\"Month-Year\")\n",
    "axes[1].set_ylabel(\"Number of Sells\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387de08",
   "metadata": {},
   "source": [
    "These Plots tells us that the user had buy the most number of the stocks in the first hour of the day and sell stocks in the last hour of the day while also says that the user was trading from the Nov,21 but were mostly active between the June,22 to Sept,22 and Oct,23 to July,24 telling that he is not focused properly or gets unactive after huge losses and also he had taken some gaps between the months were he was completely unactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd6024",
   "metadata": {},
   "source": [
    "2. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e6ec9",
   "metadata": {},
   "source": [
    "Now, let's do the bivariate analysis. Starting with the plots between the Numerical vs Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for all numerical features\n",
    "sns.pairplot(pnl[numerical_cols], diag_kind='kde', corner=False)\n",
    "plt.suptitle(\"Pairwise Relationships between Numerical Variables\", y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pnl[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Numerical Columns)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a53e22",
   "metadata": {},
   "source": [
    "---------Plots between Numerical vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = pnl.select_dtypes(include=['object']).columns\n",
    "categorical_cols_drop = categorical_cols.drop(['ISIN', 'Stock name', 'Buy date', 'Sell date'])\n",
    "target = 'Realised P&L'\n",
    "\n",
    "if target not in pnl.columns:\n",
    "    print(\"Target column 'Realised P&L' not found — skipping plots.\")\n",
    "else:\n",
    "    for col in categorical_cols_drop:\n",
    "        n_unique = pnl[col].nunique()\n",
    "        \n",
    "        if col in ['Buy Time', 'Sell Time']:\n",
    "            if col == 'Buy Time':\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "                \n",
    "                for idx, time_col in enumerate(['Buy Time', 'Sell Time']):\n",
    "                    pnl_time = pnl.copy()\n",
    "                    pnl_time[f'{time_col}_Hour'] = pd.to_datetime(pnl_time[time_col], \n",
    "                                                                   format='%H:%M:%S', errors='coerce').dt.hour\n",
    "                    \n",
    "                    hourly_data = pnl_time.groupby(f'{time_col}_Hour')[target].agg(['mean', 'count']).reset_index()\n",
    "                    hourly_data = hourly_data.sort_values(f'{time_col}_Hour')\n",
    "                    \n",
    "                    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in hourly_data['mean']]\n",
    "                    \n",
    "                    axes[idx].bar(hourly_data[f'{time_col}_Hour'], hourly_data['mean'], \n",
    "                                 color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "                    axes[idx].set_title(f\"Average {target} by {time_col} (Hourly)\", \n",
    "                                       fontsize=15, fontweight='bold', pad=15)\n",
    "                    axes[idx].set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "                    axes[idx].set_ylabel(f'Average {target}', fontsize=12, fontweight='bold')\n",
    "                    axes[idx].axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "                    axes[idx].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "                    axes[idx].spines['top'].set_visible(False)\n",
    "                    axes[idx].spines['right'].set_visible(False)\n",
    "                    axes[idx].set_xticks(hourly_data[f'{time_col}_Hour'])\n",
    "                    \n",
    "                    for i, row in hourly_data.iterrows():\n",
    "                        offset = 20 if row['mean'] > 0 else -20\n",
    "                        axes[idx].text(row[f'{time_col}_Hour'], row['mean'] + offset, \n",
    "                                      f\"n={int(row['count'])}\", \n",
    "                                      ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "                                      bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                               edgecolor='gray', alpha=0.8))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        elif col == 'Symbol':\n",
    "            fig, ax = plt.subplots(figsize=(16, 9))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.barh(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                          edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_yticks(range(len(stats)))\n",
    "            ax.set_yticklabels(stats[col], fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 20 if row['mean'] > 0 else -20\n",
    "                ax.text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                       va='center', ha='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        elif col in ['Sector', 'Industry']:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(24, 9))\n",
    "            \n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'sum', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors_mean = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['mean']]\n",
    "            bars0 = axes[0].barh(range(len(stats)), stats['mean'], color=colors_mean, \n",
    "                                alpha=0.8, edgecolor='white', linewidth=2)\n",
    "            \n",
    "            axes[0].set_yticks(range(len(stats)))\n",
    "            axes[0].set_yticklabels(stats[col], fontsize=11, fontweight='bold', rotation = 30)\n",
    "            axes[0].set_title(f'Average {target} by {col}', fontsize=14, fontweight='bold', pad=15)\n",
    "            axes[0].set_xlabel(f'Average {target}', fontsize=12, fontweight='bold')\n",
    "            axes[0].axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            axes[0].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            axes[0].spines['top'].set_visible(False)\n",
    "            axes[0].spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 50 if row['mean'] > 0 else -50\n",
    "                axes[0].text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                           va='center', ha='center', fontsize=9, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                    edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            colors_total = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['sum']]\n",
    "            bars1 = axes[1].barh(range(len(stats)), stats['sum'], color=colors_total, \n",
    "                                alpha=0.8, edgecolor='white', linewidth=2)\n",
    "            \n",
    "            axes[1].set_yticks(range(len(stats)))\n",
    "            axes[1].set_yticklabels(stats[col], fontsize=11, fontweight='bold', rotation = 30)\n",
    "            axes[1].set_title(f'Total {target} by {col}', fontsize=14, fontweight='bold', pad=15)\n",
    "            axes[1].set_xlabel(f'Total {target}', fontsize=12, fontweight='bold')\n",
    "            axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            axes[1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            axes[1].spines['top'].set_visible(False)\n",
    "            axes[1].spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 400 if row['sum'] > 0 else -400\n",
    "                axes[1].text(row['sum'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                           va='center', ha='center', fontsize=9, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.4', facecolor='white', \n",
    "                                    edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        elif col == 'Remark':\n",
    "            fig, ax = plt.subplots(figsize=(8, 7))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=False)\n",
    "            \n",
    "            colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.bar(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                         edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(range(len(stats)))\n",
    "            ax.set_xticklabels(stats[col], fontsize=11, fontweight='bold', rotation=45, ha='right')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_ylabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 50 if row['mean'] > 0 else -50\n",
    "                ax.text(i, row['mean'] + offset, f\"n={int(row['count'])}\", \n",
    "                       ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(16, 7))\n",
    "            stats = pnl.groupby(col)[target].agg(['mean', 'count']).reset_index()\n",
    "            stats = stats.sort_values('mean', ascending=True)\n",
    "            \n",
    "            colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in stats['mean']]\n",
    "            \n",
    "            bars = ax.barh(range(len(stats)), stats['mean'], color=colors, alpha=0.8, \n",
    "                          edgecolor='white', linewidth=2)\n",
    "            \n",
    "            ax.set_yticks(range(len(stats)))\n",
    "            ax.set_yticklabels(stats[col], fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Average {target} by {col}', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(f'Average {target}', fontsize=13, fontweight='bold')\n",
    "            ax.set_ylabel(col, fontsize=13, fontweight='bold')\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "            ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            for i, (idx, row) in enumerate(stats.iterrows()):\n",
    "                offset = 150 if row['mean'] > 0 else -150\n",
    "                ax.text(row['mean'] + offset, i, f\"n={int(row['count'])}\", \n",
    "                       va='center', ha='center', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n",
    "                                edgecolor='gray', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4209c",
   "metadata": {},
   "source": [
    "-----------Plots between Categorical vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_pairs = [\n",
    "    ('Buy Month', 'Sell Month'),\n",
    "    ('Buy Month', 'Sector'),\n",
    "    ('Sell Month', 'Sector'),\n",
    "    ('Remark', 'Sector'),\n",
    "    ('Remark', 'Industry'),\n",
    "    ('Buy Month', 'Remark'),\n",
    "    ('Sell Month', 'Remark')\n",
    "]\n",
    "\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "for col1, col2 in important_pairs:\n",
    "    if col1 in pnl.columns and col2 in pnl.columns:\n",
    "        if pnl[col1].nunique() <= 20 and pnl[col2].nunique() <= 20:\n",
    "            \n",
    "            if col1 in ['Buy Month', 'Sell Month']:\n",
    "                pnl_temp = pnl[pnl[col1].isin(month_order)].copy()\n",
    "                pnl_temp[col1] = pd.Categorical(pnl_temp[col1], categories=month_order, ordered=True)\n",
    "            else:\n",
    "                pnl_temp = pnl.copy()\n",
    "            \n",
    "            if col2 in ['Buy Month', 'Sell Month']:\n",
    "                pnl_temp = pnl_temp[pnl_temp[col2].isin(month_order)].copy()\n",
    "                pnl_temp[col2] = pd.Categorical(pnl_temp[col2], categories=month_order, ordered=True)\n",
    "            \n",
    "            cross_tab = pd.crosstab(pnl_temp[col1], pnl_temp[col2])\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 9))\n",
    "            \n",
    "            sns.heatmap(cross_tab, cmap='YlOrRd', annot=True, fmt='d', \n",
    "                       linewidths=2, linecolor='white',\n",
    "                       cbar_kws={'label': 'Trade Count', 'shrink': 0.8},\n",
    "                       annot_kws={'fontsize': 11, 'fontweight': 'bold'},\n",
    "                       ax=ax)\n",
    "            \n",
    "            ax.set_title(f\"Relationship between {col1} and {col2}\", \n",
    "                        fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(col2, fontsize=13, fontweight='bold', labelpad=10)\n",
    "            ax.set_ylabel(col1, fontsize=13, fontweight='bold', labelpad=10)\n",
    "            \n",
    "            ax.tick_params(axis='x', labelsize=11, rotation=45)\n",
    "            ax.tick_params(axis='y', labelsize=11, rotation=0)\n",
    "            \n",
    "            plt.setp(ax.get_xticklabels(), ha='right', rotation_mode='anchor')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a583fe",
   "metadata": {},
   "source": [
    "--------- These plots..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed598c",
   "metadata": {},
   "source": [
    "we can also use the Pandas Profiller to apply the EDA to the User Trade History dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(pnl, title=\"User Trade History Analysis\")\n",
    "profile.to_file(\"User Trade History Analysis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.info()\n",
    "print(\"\\nValue Counts for Each Column:\\n\")\n",
    "\n",
    "for col in pnl.columns:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(pnl[col].value_counts(dropna=False))  # include NaN counts\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc50dc",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ac989",
   "metadata": {},
   "source": [
    "1. Feature Construction (Manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78217225",
   "metadata": {},
   "source": [
    "Since the realised profit and loss is not calculating the charges, taxes and brokerages paid by the user to the Broker and Exchange so we will be calculating them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Base Values ---\n",
    "buy_val = pnl['Buy value']\n",
    "sell_val = pnl['Sell value']\n",
    "turnover = buy_val + sell_val  # total trade value\n",
    "\n",
    "# --- Brokerage (0.05% or ₹20 per executed order whichever is lower, both sides combined) ---\n",
    "pnl['Brokerage'] = (turnover * 0.0005).clip(upper=40)  # Max ₹20 per side (40 total for buy+sell)\n",
    "\n",
    "# --- Exchange Transaction Charges (NSE EQ: 0.00345%, BSE EQ: ~0.00297%) ---\n",
    "pnl['Exchange Charges'] = turnover * 0.0000297\n",
    "\n",
    "# --- SEBI Fees (₹10 per crore) ---\n",
    "pnl['SEBI Charges'] = turnover * 0.000001\n",
    "\n",
    "# --- IPF (Investor Protection Fund) ---\n",
    "pnl['Investor Protection Fund'] = turnover * 0.000001\n",
    "\n",
    "# --- Initialize other columns ---\n",
    "pnl['STT'] = 0.0\n",
    "pnl['Stamp Duty'] = 0.0\n",
    "pnl['Demat Charges'] = 0.0\n",
    "\n",
    "# --- Intraday Trades ---\n",
    "mask_intraday = pnl['Remark'].str.contains('Intraday', case=False, na=False)\n",
    "pnl.loc[mask_intraday, 'STT'] = sell_val * 0.00025\n",
    "pnl.loc[mask_intraday, 'Stamp Duty'] = buy_val * 0.00003\n",
    "pnl.loc[mask_intraday, 'Demat Charges'] = 0.0\n",
    "\n",
    "# --- Delivery Trades ---\n",
    "mask_delivery = ~mask_intraday & ~pnl['Remark'].str.contains('IPO', case=False, na=False)\n",
    "pnl.loc[mask_delivery, 'STT'] = sell_val * 0.001\n",
    "pnl.loc[mask_delivery, 'Stamp Duty'] = buy_val * 0.00015\n",
    "pnl.loc[mask_delivery, 'Demat Charges'] = 13.5 * 1.18  # apply GST directly (13.5 + 18%)\n",
    "\n",
    "# --- IPO Trades ---\n",
    "mask_ipo = pnl['Remark'].str.contains('IPO', case=False, na=False)\n",
    "pnl.loc[mask_ipo, 'STT'] = sell_val * 0.001\n",
    "pnl.loc[mask_ipo, 'Stamp Duty'] = 0.0\n",
    "pnl.loc[mask_ipo, 'Exchange Charges'] = sell_val * 0.0000297\n",
    "pnl.loc[mask_ipo, 'SEBI Charges'] = sell_val * 0.000001\n",
    "pnl.loc[mask_ipo, 'Investor Protection Fund'] = sell_val * 0.000001\n",
    "pnl.loc[mask_ipo, 'Demat Charges'] = 13.5 * 1.18\n",
    "\n",
    "# --- GST (18%) ---\n",
    "# Apply only on Brokerage + Exchange Charges (per GST Act)\n",
    "pnl['GST'] = (pnl['Brokerage'] + pnl['Exchange Charges']) * 0.18\n",
    "\n",
    "# --- Total Charges (Sum Everything) ---\n",
    "pnl['Total Charges'] = (\n",
    "    pnl['Brokerage'] +\n",
    "    pnl['Exchange Charges'] +\n",
    "    pnl['SEBI Charges'] +\n",
    "    pnl['Investor Protection Fund'] +\n",
    "    pnl['STT'] +\n",
    "    pnl['Stamp Duty'] +\n",
    "    pnl['Demat Charges'] +\n",
    "    pnl['GST']\n",
    ").round(2)\n",
    "\n",
    "pnl['Total Charges'].sum().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c730036",
   "metadata": {},
   "source": [
    "Now, let's find the Gross and Net pnl and pnl % to better understand the profits and losses. This will give the idea of overall profits user generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aaf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Gross and Net pnl and pnl %\n",
    "pnl['Gross Realised P&L'] = pnl['Sell value'] - pnl['Buy value']\n",
    "pnl['Gross Realised PnL %'] = (pnl['Gross Realised P&L'] / pnl['Buy value']) * 100\n",
    "pnl['Net Realised P&L'] = pnl['Gross Realised P&L'] - pnl['Total Charges']\n",
    "pnl['Net Realised PnL %'] = (pnl['Net Realised P&L'] / pnl['Buy value']) * 100\n",
    "pnl = pnl.drop(columns= 'Realised P&L')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ad2fd",
   "metadata": {},
   "source": [
    "Now, let's find the holding period to understand the user's preference and find the most profitable holding period for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both date columns are proper datetime objects\n",
    "pnl['Buy date'] = pd.to_datetime(pnl['Buy date'], errors='coerce')\n",
    "pnl['Sell date'] = pd.to_datetime(pnl['Sell date'], errors='coerce')\n",
    "\n",
    "# Now safely calculate holding period\n",
    "pnl['Holding Period (Days)'] = (pnl['Sell date'] - pnl['Buy date']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6b15c",
   "metadata": {},
   "source": [
    "2. Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e2b0d",
   "metadata": {},
   "source": [
    "Since the name of the stock, ISIN value and the Symbol are all represents the same thing we can drop the Stock name and ISIN value features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed469cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns (repetitive or identifiers)\n",
    "pnl = pnl.drop(columns=['Stock name', 'ISIN', 'Symbol'], errors='ignore')\n",
    "\n",
    "# Drop any duplicate records if exist\n",
    "pnl = pnl.drop_duplicates()\n",
    "\n",
    "# Create additional ratio features (optional but useful for ML)\n",
    "pnl['PnL per Trade Value'] = pnl['Net Realised P&L'] / (pnl['Buy value'] + pnl['Sell value'])\n",
    "pnl['Buy/Sell Price Ratio'] = pnl['Buy price'] / pnl['Sell price']\n",
    "\n",
    "# Replace infinite or NaN values with 0\n",
    "pnl = pnl.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "pnl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba2e03",
   "metadata": {},
   "source": [
    "Merging the Industry and Sector rows which are not appearing frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_industry = pnl['Industry'].value_counts() \n",
    "pnl['Industry'] = pnl['Industry'].replace(uncommon_industry[uncommon_industry < 6].index, 'Others')\n",
    "\n",
    "uncommon_Sector = pnl['Sector'].value_counts() \n",
    "pnl['Sector'] = pnl['Sector'].replace(uncommon_Sector[uncommon_Sector <= 5].index, 'Others')\n",
    "\n",
    "pnl['Industry'].value_counts()\n",
    "pnl['Sector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl[['Buy date', 'Buy Day', 'Buy Day of Week', 'Buy Month', 'Buy Year', 'Buy Time', 'Buy Interval', 'Buy Hour', 'Buy Time Minutes', 'Buy Hour Decimal']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aefa09",
   "metadata": {},
   "source": [
    "Now, let's do the train test split the data to apply the scaling and encoding to the train split and apply that same transformation to the test split. Let's encode and scale the features before applying the ML Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl.shape\n",
    "\n",
    "['Quantity', 'Buy price', 'Buy value', 'Sell price', 'Sell value', 'Remark', 'Sector', 'Industry', 'Buy Year', 'Buy Month', 'Buy Day', 'Buy Day of Week', 'Sell Year', 'Sell Month', 'Sell Day', 'Sell Day of Week', 'Buy Time Minutes', 'Sell Time Minutes', 'Total Charges', 'Holding Period (Days)', 'PnL per Trade Value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Step 1: Reinitialize encoders with proper settings\n",
    "ss = StandardScaler()\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Step 2: Remove target and leakage columns\n",
    "\n",
    "X = pnl[['Quantity', 'Buy price', 'Sell price', 'Remark', 'Sector', 'Industry', 'Buy Year', 'Buy Month', 'Buy Day', 'Buy Day of Week', 'Sell Year', 'Sell Month', 'Sell Day', 'Sell Day of Week', 'Buy Time Minutes', 'Sell Time Minutes', 'Total Charges', 'Holding Period (Days)', 'PnL per Trade Value']]\n",
    "\n",
    "y = pnl['Net Realised P&L']\n",
    "\n",
    "# Step 3: Check which columns actually exist in X\n",
    "print(\"Available columns in X:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Step 4: Define transformers based on existing columns\n",
    "# Filter to only existing columns\n",
    "\n",
    "numerical_cols = ['Quantity', 'Buy price', 'Buy value', 'Sell price', 'Sell value', 'Total Charges', 'Holding Period (Days)']\n",
    "numerical_cols = [col for col in numerical_cols if col in X.columns]\n",
    "\n",
    "ordinal_cols = ['Buy Year', 'Buy Month', 'Buy Day', 'Buy Day of Week', \n",
    "                'Sell Year', 'Sell Month', 'Sell Day', 'Sell Day of Week']\n",
    "ordinal_cols = [col for col in ordinal_cols if col in X.columns]\n",
    "\n",
    "categorical_cols = ['Remark', 'Sector', 'Industry']\n",
    "categorical_cols = [col for col in categorical_cols if col in X.columns]\n",
    "\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Ordinal columns ({len(ordinal_cols)}): {ordinal_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d21be",
   "metadata": {},
   "source": [
    "Model------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b12da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "trfs = [\n",
    "    ('ohe', ohe, categorical_cols),\n",
    "    ('oe', oe, ordinal_cols),\n",
    "    ('ss', ss, numerical_cols)\n",
    "]\n",
    "\n",
    "ct = ColumnTransformer(transformers=trfs, remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "X_train_trf = ct.fit_transform(X_train)\n",
    "X_test_trf = ct.transform(X_test)\n",
    "\n",
    "X_train_trf = pd.DataFrame(X_train_trf, columns=ct.get_feature_names_out())\n",
    "X_test_trf = pd.DataFrame(X_test_trf, columns=ct.get_feature_names_out())\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL TRAINING AND EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_trf, y_train)\n",
    "    y_pred = model.predict(X_test_trf)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test.values - y_pred) / np.where(y_test.values != 0, y_test.values, 1))) * 100\n",
    "    \n",
    "    results.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R²': r2, 'MAPE': mape})\n",
    "    trained_models[name] = model\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('R²', ascending=False)\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_r2 = results_df.iloc[0]['R²']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_model_name} (R² = {best_r2:.4f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test_trf)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.5, s=20)\n",
    "    axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[idx].set_xlabel('Actual Net P&L')\n",
    "    axes[idx].set_ylabel('Predicted Net P&L')\n",
    "    axes[idx].set_title(f'{name} (R²={r2:.4f})')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']\n",
    "if best_model_name in tree_models:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train_trf.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Features:\")\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = 20\n",
    "    plt.barh(feature_importance['Feature'][:top_n], feature_importance['Importance'][:top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Features - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SHAP ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if best_model_name in tree_models:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    sample_size = min(100, len(X_test_trf))\n",
    "    X_sample = X_test_trf.sample(n=sample_size, random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "else:\n",
    "    background = shap.sample(X_train_trf, 100)\n",
    "    explainer = shap.KernelExplainer(best_model.predict, background)\n",
    "    sample_size = min(50, len(X_test_trf))\n",
    "    X_sample = X_test_trf.sample(n=sample_size, random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': X_sample.columns,\n",
    "    'SHAP_Importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('SHAP_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features by SHAP Importance:\")\n",
    "print(shap_importance.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"📊 WHAT THE MODEL ACTUALLY LEARNED (In Plain English)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "top_features_shap = shap_importance.head(10)\n",
    "\n",
    "print(\"🔍 ANALYZING YOUR TRADING PATTERNS...\\n\")\n",
    "\n",
    "for rank, row in enumerate(top_features_shap.iterrows(), 1):\n",
    "    idx, data = row\n",
    "    feature = data['Feature']\n",
    "    importance = data['SHAP_Importance']\n",
    "    \n",
    "    feature_idx = list(X_sample.columns).index(feature)\n",
    "    feature_shap = shap_values[:, feature_idx]\n",
    "    feature_values = X_sample[feature].values\n",
    "    \n",
    "    avg_positive = feature_shap[feature_shap > 0].mean() if (feature_shap > 0).any() else 0\n",
    "    avg_negative = feature_shap[feature_shap < 0].mean() if (feature_shap < 0).any() else 0\n",
    "    correlation = np.corrcoef(feature_values, feature_shap)[0, 1]\n",
    "    \n",
    "    print(f\"{rank}. **{feature.upper()}**\")\n",
    "    \n",
    "    if 'PnL per Trade Value' in feature or 'PNL' in feature:\n",
    "        print(f\"   💡 This is basically measuring your trade size relative to profit.\")\n",
    "        print(f\"   📈 Impact: MASSIVE (score: {importance:.1f})\")\n",
    "        print(f\"   🎯 What it means: Your profit per rupee invested is THE most important factor.\")\n",
    "        print(f\"      → Better risk-reward ratios = better overall P&L\")\n",
    "    \n",
    "    elif 'Buy Time' in feature or 'Sell Time' in feature:\n",
    "        time_type = \"ENTRY\" if 'Buy' in feature else \"EXIT\"\n",
    "        print(f\"   ⏰ Your {time_type} timing matters a LOT (score: {importance:.1f})\")\n",
    "        \n",
    "        if correlation > 0.1:\n",
    "            print(f\"   📈 Pattern detected: Later in the day = MORE profit\")\n",
    "            if 'Buy' in feature:\n",
    "                print(f\"      → You make better decisions after market opens\")\n",
    "                print(f\"      → Morning rush trades are hurting you\")\n",
    "            else:\n",
    "                print(f\"      → Holding till later improves your exits\")\n",
    "        elif correlation < -0.1:\n",
    "            print(f\"   📉 Pattern detected: Early timing = MORE profit\")\n",
    "            if 'Buy' in feature:\n",
    "                print(f\"      → Your best entries are in the first 30-60 minutes\")\n",
    "                print(f\"      → Late entries are costing you money\")\n",
    "            else:\n",
    "                print(f\"      → Quick exits work better for your style\")\n",
    "        else:\n",
    "            print(f\"   ⚖️ Timing effect is MIXED - depends on the specific trade\")\n",
    "    \n",
    "    elif 'Holding Period' in feature:\n",
    "        print(f\"   📅 How long you hold positions (score: {importance:.1f})\")\n",
    "        if correlation > 0.15:\n",
    "            print(f\"   📈 CLEAR PATTERN: Longer holds = Better profits\")\n",
    "            print(f\"      → You're exiting too early!\")\n",
    "            print(f\"      → Let your winners run - patience pays off\")\n",
    "        elif correlation < -0.15:\n",
    "            print(f\"   📉 CLEAR PATTERN: Quick trades = Better profits\")\n",
    "            print(f\"      → You're a scalper/day trader at heart\")\n",
    "            print(f\"      → Holding overnight is reducing returns\")\n",
    "        else:\n",
    "            print(f\"   ⚖️ No clear pattern - holding period varies by trade type\")\n",
    "    \n",
    "    elif 'Total Charges' in feature or 'Brokerage' in feature or 'STT' in feature:\n",
    "        print(f\"   💸 Trading costs are KILLING your profits (score: {importance:.1f})\")\n",
    "        print(f\"   💰 Average damage: ₹{abs(avg_negative):.2f} per trade\")\n",
    "        print(f\"   ⚠️ KEY INSIGHT: You're overtrading or using expensive broker\")\n",
    "        print(f\"      → Reduce trade frequency\")\n",
    "        print(f\"      → Switch to discount broker if possible\")\n",
    "    \n",
    "    elif 'Buy price' in feature or 'Sell price' in feature:\n",
    "        print(f\"   💵 Price levels matter (score: {importance:.1f})\")\n",
    "        if 'Buy' in feature:\n",
    "            if correlation < 0:\n",
    "                print(f\"   📉 Pattern: Lower buy prices = Better returns\")\n",
    "                print(f\"      → You're good at buying dips\")\n",
    "            else:\n",
    "                print(f\"   📈 Pattern: Higher buy prices = Better returns\")\n",
    "                print(f\"      → Momentum trading works for you\")\n",
    "        else:\n",
    "            print(f\"   📊 Exit price quality affects P&L significantly\")\n",
    "    \n",
    "    elif 'Sector' in feature:\n",
    "        sector_name = feature.replace('ohe_', '').replace('Sector_', '').replace('_', ' ')\n",
    "        print(f\"   🏢 {sector_name.upper()} sector performance (score: {importance:.1f})\")\n",
    "        if avg_positive > abs(avg_negative):\n",
    "            print(f\"   ✅ This sector is PROFITABLE for you (+₹{avg_positive:.2f} avg)\")\n",
    "            print(f\"      → Trade more in this sector\")\n",
    "        else:\n",
    "            print(f\"   ❌ This sector is HURTING you (-₹{abs(avg_negative):.2f} avg)\")\n",
    "            print(f\"      → Avoid or reduce exposure here\")\n",
    "    \n",
    "    elif 'Industry' in feature:\n",
    "        industry_name = feature.replace('ohe_', '').replace('Industry_', '').replace('_', ' ')\n",
    "        print(f\"   🏭 {industry_name.upper()} industry (score: {importance:.1f})\")\n",
    "        if avg_positive > abs(avg_negative):\n",
    "            print(f\"   ✅ Strong performance: +₹{avg_positive:.2f} average\")\n",
    "        else:\n",
    "            print(f\"   ❌ Weak performance: -₹{abs(avg_negative):.2f} average\")\n",
    "    \n",
    "    elif 'Quantity' in feature:\n",
    "        print(f\"   📦 Position size impact (score: {importance:.1f})\")\n",
    "        print(f\"   💡 Larger positions amplify both wins AND losses\")\n",
    "        if correlation > 0:\n",
    "            print(f\"      → Bigger positions = better overall returns\")\n",
    "            print(f\"      → You might be sizing too small\")\n",
    "        else:\n",
    "            print(f\"      → Bigger positions = worse returns\")\n",
    "            print(f\"      → You're over-leveraging\")\n",
    "    \n",
    "    elif 'Day of Week' in feature:\n",
    "        day_name = feature.split('_')[-1] if '_' in feature else 'Unknown'\n",
    "        print(f\"   📆 {day_name} trading pattern (score: {importance:.1f})\")\n",
    "        if avg_positive > 30:\n",
    "            print(f\"   ✅ BEST DAY: Average +₹{avg_positive:.2f}\")\n",
    "        elif avg_negative < -30:\n",
    "            print(f\"   ❌ WORST DAY: Average -₹{abs(avg_negative):.2f}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"   📊 Significant factor (score: {importance:.1f})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"🎯 FINAL ACTIONABLE INSIGHTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "charges_features = [f for f in shap_importance['Feature'] if 'Charges' in f or 'Brokerage' in f or 'STT' in f]\n",
    "if charges_features:\n",
    "    total_charges_impact = shap_importance[shap_importance['Feature'].isin(charges_features)]['SHAP_Importance'].sum()\n",
    "    if total_charges_impact > 15:\n",
    "        insights.append(\"💸 **Your losses mainly come from high brokerage/charges.**\")\n",
    "        insights.append(\"   → Switch to discount broker or reduce trade frequency\\n\")\n",
    "\n",
    "buy_time_features = [f for f in X_sample.columns if 'Buy' in f and ('Time' in f or 'Hour' in f or 'Interval' in f)]\n",
    "if buy_time_features:\n",
    "    feature_idx = list(X_sample.columns).index(buy_time_features[0])\n",
    "    time_values = X_sample.iloc[:, feature_idx].values\n",
    "    time_shap = shap_values[:, feature_idx]\n",
    "    \n",
    "    early_trades = time_values < np.percentile(time_values, 33)\n",
    "    late_trades = time_values > np.percentile(time_values, 67)\n",
    "    \n",
    "    early_impact = time_shap[early_trades].mean()\n",
    "    late_impact = time_shap[late_trades].mean()\n",
    "    \n",
    "    if early_impact > late_impact + 20:\n",
    "        insights.append(\"⏰ **You consistently profit more in early morning trades.**\")\n",
    "        insights.append(\"   → Focus on first 1-2 hours after market open\\n\")\n",
    "    elif late_impact > early_impact + 20:\n",
    "        insights.append(\"⏰ **Afternoon trades show better profitability.**\")\n",
    "        insights.append(\"   → Avoid morning rush, trade after 11 AM\\n\")\n",
    "\n",
    "sector_features = [f for f in shap_importance['Feature'][:10] if 'Sector' in f or 'Industry' in f]\n",
    "if len(sector_features) >= 2:\n",
    "    insights.append(\"🏢 **Certain sectors are hurting your P&L.**\")\n",
    "    insights.append(\"   → Check which sectors have negative SHAP values above\\n\")\n",
    "\n",
    "holding_features = [f for f in X_sample.columns if 'Holding Period' in f]\n",
    "if holding_features:\n",
    "    hp_idx = list(X_sample.columns).index(holding_features[0])\n",
    "    hp_corr = np.corrcoef(X_sample.iloc[:, hp_idx].values, shap_values[:, hp_idx])[0, 1]\n",
    "    \n",
    "    if hp_corr < -0.2:\n",
    "        insights.append(\"⏱️ **Holding too long is reducing your profits.**\")\n",
    "        insights.append(\"   → Exit faster, don't let winners turn into losers\\n\")\n",
    "    elif hp_corr > 0.2:\n",
    "        insights.append(\"⏱️ **You exit too early - longer holds improve returns.**\")\n",
    "        insights.append(\"   → Be patient, let trends play out\\n\")\n",
    "\n",
    "if insights:\n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "else:\n",
    "    print(\"📊 Model shows mixed patterns - profitability depends on combination of factors\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✅ ANALYSIS COMPLETE - Now you know EXACTLY what drives your P&L!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title(\"SHAP Feature Importance (Mean |SHAP value|)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "shap.summary_plot(shap_values, X_sample, show=False, max_display=20)\n",
    "plt.title(\"SHAP Impact Analysis (Red=High Feature Value, Blue=Low Feature Value)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_features = shap_importance.head(4)['Feature'].tolist()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    feature_idx = list(X_sample.columns).index(feature)\n",
    "    shap.dependence_plot(feature_idx, shap_values, X_sample, ax=axes[idx], show=False)\n",
    "    axes[idx].set_title(f\"SHAP Dependence: {feature}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_values[sample_idx], \n",
    "                                      base_values=explainer.expected_value,\n",
    "                                      data=X_sample.iloc[sample_idx],\n",
    "                                      feature_names=X_sample.columns.tolist()),\n",
    "                    max_display=15, show=False)\n",
    "plt.title(f\"SHAP Waterfall Plot - Single Prediction Explanation\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl')\n",
    "joblib.dump(ct, 'column_transformer.pkl')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL DEPLOYMENT READY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"R² Score: {best_r2:.4f}\")\n",
    "print(f\"Files Saved:\")\n",
    "print(f\"  - best_model_{best_model_name.replace(' ', '_').lower()}.pkl\")\n",
    "print(f\"  - column_transformer.pkl\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33043032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insights generator: plug this in and run after you have\n",
    "# shap_importance (DataFrame), shap_values (shap.Explanation or np.array), X_sample (DataFrame)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "def generate_detailed_insights(shap_importance, shap_values, X_sample, top_n=15):\n",
    "    \"\"\"\n",
    "    Prints extremely detailed, user-friendly insights derived from SHAP output.\n",
    "    - shap_importance: DataFrame with columns ['Feature','SHAP_Importance'] (sorted desc)\n",
    "    - shap_values: shap.Explanation object OR numpy array (samples x features)\n",
    "    - X_sample: DataFrame used to compute SHAP values (columns align with shap values)\n",
    "    - top_n: how many top features to analyze in detail\n",
    "    \"\"\"\n",
    "    # Normalize shap_values to numeric array\n",
    "    if hasattr(shap_values, \"values\"):\n",
    "        shap_array = np.array(shap_values.values)\n",
    "    else:\n",
    "        shap_array = np.array(shap_values)\n",
    "    # Safety checks\n",
    "    assert shap_array.shape[1] == X_sample.shape[1], \"Feature count mismatch between shap_values and X_sample\"\n",
    "\n",
    "    # Select top features\n",
    "    top_df = shap_importance.head(top_n).reset_index(drop=True)\n",
    "\n",
    "    sep = \"=\" * 80\n",
    "    print(f\"\\n{sep}\")\n",
    "    print(\"📊 WHAT THE MODEL ACTUALLY LEARNED (IN PLAIN ENGLISH)\")\n",
    "    print(sep + \"\\n\")\n",
    "\n",
    "    print(\"🔍 ANALYZING YOUR TRADING PATTERNS...\\n\")\n",
    "\n",
    "    def safe_corr(a, b):\n",
    "        try:\n",
    "            if np.std(a) == 0 or np.std(b) == 0:\n",
    "                return 0.0\n",
    "            c = np.corrcoef(a, b)[0, 1]\n",
    "            if np.isnan(c):\n",
    "                return 0.0\n",
    "            return float(c)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def clean_name(name):\n",
    "        # make feature display name human friendly\n",
    "        s = name\n",
    "        s = s.replace(\"ohe_\", \"\").replace(\"oe_\", \"\")\n",
    "        s = s.replace(\"_\", \" \").replace(\".\", \" \").strip()\n",
    "        return s\n",
    "\n",
    "    for rank, row in top_df.iterrows():\n",
    "        feature = row['Feature']\n",
    "        impact = row['SHAP_Importance']\n",
    "        try:\n",
    "            feat_idx = list(X_sample.columns).index(feature)\n",
    "        except ValueError:\n",
    "            # fallback attempt: match by cleaned name\n",
    "            matches = [i for i, c in enumerate(X_sample.columns) if feature.lower() in c.lower()]\n",
    "            feat_idx = matches[0] if matches else None\n",
    "\n",
    "        if feat_idx is None:\n",
    "            continue\n",
    "\n",
    "        feature_shap = shap_array[:, feat_idx]\n",
    "        feature_values = X_sample.iloc[:, feat_idx].values\n",
    "\n",
    "        avg_pos = feature_shap[feature_shap > 0].mean() if (feature_shap > 0).any() else 0.0\n",
    "        avg_neg = feature_shap[feature_shap < 0].mean() if (feature_shap < 0).any() else 0.0\n",
    "        corr = safe_corr(feature_values.astype(float, copy=False), feature_shap)\n",
    "\n",
    "        display_name = clean_name(feature)\n",
    "        print(f\"{rank+1}. {display_name.upper()} (impact score: {impact:.4f})\")\n",
    "\n",
    "        # Core interpretation rules (prioritized)\n",
    "        lname = display_name.lower()\n",
    "\n",
    "        # PnL and trade-value related features\n",
    "        if 'pnl' in lname or 'per trade value' in lname or 'trade value' in lname:\n",
    "            print(textwrap.indent(\n",
    "                \"• What this means: This feature measures profit scaled to trade size. \"\n",
    "                \"High importance here means your returns are strongly tied to how much capital you risk per trade.\",\n",
    "                '   '))\n",
    "            print(textwrap.indent(f\"• Typical effect: Avg positive contribution = ₹{avg_pos:.2f}, avg negative = ₹{abs(avg_neg):.2f}\", '   '))\n",
    "            print(textwrap.indent(\"• Suggested action: normalize position sizing, compute expected P&L per unit exposure, and avoid very small-profit setups where charges eat gains.\", '   '))\n",
    "\n",
    "        # Time features (Buy/Sell/Hour/Interval)\n",
    "        elif any(k in lname for k in ['buy time', 'sell time', 'buy hour', 'sell hour', 'interval', 'hour']):\n",
    "            which = \"ENTRY (buy)\" if 'buy' in lname else (\"EXIT (sell)\" if 'sell' in lname else \"TIME\")\n",
    "            print(textwrap.indent(f\"• What this means: {which} timing influences outcomes. SHAP shows timing changes push predictions up/down.\", '   '))\n",
    "            print(textwrap.indent(f\"• Correlation with SHAP: {corr:.3f}\", '   '))\n",
    "            if corr > 0.12:\n",
    "                print(textwrap.indent(\"• Pattern: Later times tend to increase expected P&L for these trades. You might be better at letting the market settle before entering/exiting.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Shift more entries/exits into the later time buckets where your historical edge exists.\", '   '))\n",
    "            elif corr < -0.12:\n",
    "                print(textwrap.indent(\"• Pattern: Earlier times correlate with higher expected P&L — your edge is in the open/first-hour moves.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Prioritize first-hour setups and reduce trades in low-edge hours.\", '   '))\n",
    "            else:\n",
    "                print(textwrap.indent(\"• Pattern: Mixed effect — timing matters in combination with other features (e.g., sector, charges).\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Use a rule: only trade at these times if other conditions (sector, charges) are favorable.\", '   '))\n",
    "\n",
    "        # Holding period\n",
    "        elif 'holding' in lname or 'hold' in lname:\n",
    "            print(textwrap.indent(\"• What this means: How long you keep a position changes profit expectation.\", '   '))\n",
    "            print(textwrap.indent(f\"• Correlation with SHAP: {corr:.3f}\", '   '))\n",
    "            if corr > 0.18:\n",
    "                print(textwrap.indent(\"• Pattern: Longer holds = better profits. You currently cut winners too early.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Consider trailing stops and give winning trades more runway.\", '   '))\n",
    "            elif corr < -0.18:\n",
    "                print(textwrap.indent(\"• Pattern: Short/quick exits = better profits. You are effectively a scalper.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Avoid overnight or multi-day holds unless the edge is strong.\", '   '))\n",
    "            else:\n",
    "                print(textwrap.indent(\"• Pattern: No single holding duration dominates — success depends on trade context.\", '   '))\n",
    "\n",
    "        # Charges related\n",
    "        elif any(k in lname for k in ['charge', 'brokerage', 'stt', 'gst', 'total charges', 'stamp duty', 'demat']):\n",
    "            print(textwrap.indent(\"• What this means: Transaction costs are directly reducing your Net P&L.\", '   '))\n",
    "            print(textwrap.indent(f\"• Typical drag: avg negative SHAP contribution = ₹{abs(avg_neg):.2f}\", '   '))\n",
    "            print(textwrap.indent(\"• Pattern: Even otherwise profitable trades often flip to losses after charges.\", '   '))\n",
    "            print(textwrap.indent(\"• Action: (1) Reduce trade frequency, (2) increase per-trade target to justify cost, (3) consider a lower-cost broker for scalping.\", '   '))\n",
    "\n",
    "        # Price-related\n",
    "        elif any(k in lname for k in ['buy price', 'sell price', 'price ratio', 'buy/sell price', 'price']):\n",
    "            print(textwrap.indent(\"• What this means: Entry/exit price levels strongly affect realized return.\", '   '))\n",
    "            if 'buy' in lname:\n",
    "                if corr < -0.12:\n",
    "                    print(textwrap.indent(\"• Pattern: Lower buy price (buying dips) yields better returns.\", '   '))\n",
    "                    print(textwrap.indent(\"• Action: Focus on confirmed support zones and use limit entries.\", '   '))\n",
    "                elif corr > 0.12:\n",
    "                    print(textwrap.indent(\"• Pattern: Higher buy prices (momentum entries) work better for you.\", '   '))\n",
    "                    print(textwrap.indent(\"• Action: Use breakout confirmation rules; avoid buying weak pullbacks.\", '   '))\n",
    "                else:\n",
    "                    print(textwrap.indent(\"• Pattern: Mixed — combine price signals with momentum/volume filters.\", '   '))\n",
    "            else:\n",
    "                print(textwrap.indent(\"• Exit price quality strongly determines P&L — good exits = bigger realized profits.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Improve exit rules (targets + trailing stops) and avoid emotional exits.\", '   '))\n",
    "\n",
    "        # Sector / Industry\n",
    "        elif 'sector' in lname or 'industry' in lname:\n",
    "            pretty = display_name.replace('sector', '').replace('industry', '').strip()\n",
    "            print(textwrap.indent(f\"• What this means: This categorical feature measures sector/industry-specific performance.\", '   '))\n",
    "            print(textwrap.indent(f\"• Avg contribution when positive = ₹{avg_pos:.2f}; when negative = ₹{abs(avg_neg):.2f}\", '   '))\n",
    "            if abs(avg_pos) > abs(avg_neg):\n",
    "                print(textwrap.indent(f\"• Pattern: {pretty} tends to be profitable for you historically.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Consider increasing exposure or allocating a focused watchlist for this sector.\", '   '))\n",
    "            else:\n",
    "                print(textwrap.indent(f\"• Pattern: {pretty} underperforms — it often reduces your Net P&L.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Reduce position size or avoid this sector unless setup quality is exceptional.\", '   '))\n",
    "\n",
    "        # Quantity\n",
    "        elif 'quantity' in lname or 'position size' in lname:\n",
    "            print(textwrap.indent(\"• What this means: Position size amplifies both profit and loss.\", '   '))\n",
    "            print(textwrap.indent(f\"• Correlation with SHAP: {corr:.3f}\", '   '))\n",
    "            if corr > 0.08:\n",
    "                print(textwrap.indent(\"• Pattern: Larger sizes historically correlated with better returns — you size up when confident.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Keep a disciplined sizing rule (e.g., % of equity or volatility-based sizing).\", '   '))\n",
    "            else:\n",
    "                print(textwrap.indent(\"• Pattern: Larger sizes amplify mistakes and often lead to worse outcomes.\", '   '))\n",
    "                print(textwrap.indent(\"• Action: Cap max position size and scale into positions instead of going all-in.\", '   '))\n",
    "\n",
    "        # Day of week / other\n",
    "        elif 'day' in lname or 'weekday' in lname:\n",
    "            print(textwrap.indent(\"• What this means: The day of week correlates with your edge (emotional cycles, news cadence).\", '   '))\n",
    "            print(textwrap.indent(f\"• Avg positive contribution = ₹{avg_pos:.2f}; avg negative = ₹{abs(avg_neg):.2f}\", '   '))\n",
    "            print(textwrap.indent(\"• Action: Track day-wise P&L and avoid low-edge days.\", '   '))\n",
    "\n",
    "        else:\n",
    "            # Generic fallback\n",
    "            print(textwrap.indent(\"• What this means: This feature significantly influences predicted P&L in your history.\", '   '))\n",
    "            print(textwrap.indent(f\"• Avg positive contribution = ₹{avg_pos:.2f}; avg negative = ₹{abs(avg_neg):.2f}\", '   '))\n",
    "            print(textwrap.indent(\"• Action: Investigate this feature in combination with others to form rules (e.g., time + sector + charges).\", '   '))\n",
    "\n",
    "        print()  # blank line between features\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregated actionable insights\n",
    "    # -------------------------\n",
    "    print(sep)\n",
    "    print(\"🎯 FINAL ACTIONABLE INSIGHTS (PRIORITIZED)\")\n",
    "    print(sep + \"\\n\")\n",
    "\n",
    "    insights = []\n",
    "\n",
    "    # Charges summary\n",
    "    charges_keys = [f for f in shap_importance['Feature'] if any(k in f.lower() for k in ['charge', 'brokerage', 'stt', 'gst', 'total charges', 'stamp', 'demat'])]\n",
    "    if charges_keys:\n",
    "        ch_imp = shap_importance[shap_importance['Feature'].isin(charges_keys)]['SHAP_Importance'].sum()\n",
    "        if ch_imp > 0.8 * shap_importance['SHAP_Importance'].sum() * 0.15:  # heuristic\n",
    "            insights.append((\"Costs are a major drag\", [\n",
    "                \"Your average realized profit is being reduced materially by transaction costs.\",\n",
    "                \"Prioritize trades where expected gross P&L >> total charges (target net margin).\",\n",
    "                \"For scalping, charges must be < X% of expected edge — otherwise skip.\",\n",
    "                \"Consider a lower-cost broker or reduce trade frequency.\"\n",
    "            ]))\n",
    "\n",
    "    # Timing summary (use first buy-time-like column if exists)\n",
    "    buy_time_cols = [c for c in X_sample.columns if 'buy' in c.lower() and ('time' in c.lower() or 'hour' in c.lower() or 'interval' in c.lower())]\n",
    "    if buy_time_cols:\n",
    "        idx = X_sample.columns.tolist().index(buy_time_cols[0])\n",
    "        time_vals = X_sample.iloc[:, idx].values\n",
    "        time_shap = shap_array[:, idx]\n",
    "        early_mask = time_vals <= np.percentile(time_vals, 33)\n",
    "        late_mask = time_vals >= np.percentile(time_vals, 67)\n",
    "        early_avg = time_shap[early_mask].mean() if early_mask.any() else 0\n",
    "        late_avg = time_shap[late_mask].mean() if late_mask.any() else 0\n",
    "        if early_avg > late_avg + abs(early_avg)*0.2:\n",
    "            insights.append((\"Morning edge\", [\n",
    "                \"You perform best in the morning window.\",\n",
    "                \"Allocate more capital and focus to the first 60–90 minutes of trading.\",\n",
    "                \"Avoid late-day trades unless other conditions are ideal.\"\n",
    "            ]))\n",
    "        elif late_avg > early_avg + abs(late_avg)*0.2:\n",
    "            insights.append((\"Afternoon edge\", [\n",
    "                \"You perform better later in the session — market settlement benefits you.\",\n",
    "                \"Consider filters that reduce early volatility exposure.\",\n",
    "                \"Schedule watchlists for setups after 11:00 AM.\"\n",
    "            ]))\n",
    "\n",
    "    # Holding period summary\n",
    "    hold_cols = [c for c in X_sample.columns if 'holding' in c.lower() or 'hold' in c.lower()]\n",
    "    if hold_cols:\n",
    "        idx = X_sample.columns.tolist().index(hold_cols[0])\n",
    "        hp_corr = safe_corr(X_sample.iloc[:, idx].values.astype(float), shap_array[:, idx])\n",
    "        if hp_corr > 0.18:\n",
    "            insights.append((\"Be more patient with winners\", [\n",
    "                \"Longer holds tend to increase profits — you're likely cutting winners early.\",\n",
    "                \"Use trailing stops that scale out over time rather than fixed tight targets.\",\n",
    "                \"Backtest a 2x–3x holding horizon to see improvement in realized gains.\"\n",
    "            ]))\n",
    "        elif hp_corr < -0.18:\n",
    "            insights.append((\"Shorter holds suit you\", [\n",
    "                \"You are better at short-term scalps; long holds reduce performance.\",\n",
    "                \"Avoid overnight positions and favor intraday strategies.\",\n",
    "                \"Keep targets tight and exits mechanical.\"\n",
    "            ]))\n",
    "\n",
    "    # Sector summary\n",
    "    sector_feats = [f for f in shap_importance['Feature'] if 'sector' in f.lower() or 'industry' in f.lower()]\n",
    "    if sector_feats:\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for f in sector_feats:\n",
    "            try:\n",
    "                i = list(X_sample.columns).index(f)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            shp = shap_array[:, i]\n",
    "            if np.nanmean(shp[shp>0]) > abs(np.nanmean(shp[shp<0])):\n",
    "                pos.append(f)\n",
    "            else:\n",
    "                neg.append(f)\n",
    "        if neg:\n",
    "            insights.append((\"Sector caution\", [\n",
    "                f\"Some sectors (e.g., {', '.join([clean_name(x) for x in neg[:3]])}) have negative historic impact.\",\n",
    "                \"Limit exposure or require stricter entry rules in these sectors.\",\n",
    "                \"Build sector-specific rules or skip these sectors unless the setup is exceptionally strong.\"\n",
    "            ]))\n",
    "        if pos:\n",
    "            insights.append((\"Sector opportunity\", [\n",
    "                f\"Some sectors (e.g., {', '.join([clean_name(x) for x in pos[:3]])}) have historically helped your returns.\",\n",
    "                \"Create a watchlist focused on these sectors and optimize position sizing around them.\"\n",
    "            ]))\n",
    "\n",
    "    # Quantity / sizing\n",
    "    qty_cols = [c for c in X_sample.columns if 'quantity' in c.lower() or 'position' in c.lower() or 'size' in c.lower()]\n",
    "    if qty_cols:\n",
    "        idx = list(X_sample.columns).index(qty_cols[0])\n",
    "        corr_qty = safe_corr(X_sample.iloc[:, idx].values.astype(float), shap_array[:, idx])\n",
    "        if corr_qty < -0.1:\n",
    "            insights.append((\"Size discipline needed\", [\n",
    "                \"Larger sizes amplify losses — enforce max position size limits.\",\n",
    "                \"Use volatility-adjusted sizing or fixed percent risk per trade.\"\n",
    "            ]))\n",
    "        else:\n",
    "            insights.append((\"Sizing works\", [\n",
    "                \"Your sizing tends to correlate with higher returns — continue disciplined scaling.\"\n",
    "            ]))\n",
    "\n",
    "    # Print prioritized insights\n",
    "    if insights:\n",
    "        for title, bullets in insights:\n",
    "            print(f\"🔸 {title.upper()}\")\n",
    "            for b in bullets:\n",
    "                print(\"   -\", b)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"📊 Model shows mixed patterns — no single high-confidence recommendation. Consider collecting more data and re-evaluating after fresh trades.\\n\")\n",
    "\n",
    "    print(sep)\n",
    "    print(\"✅ ANALYSIS COMPLETE — these are concrete, prioritized actions you can apply today.\")\n",
    "    print(sep + \"\\n\")\n",
    "\n",
    "    # Return structured insights for programmatic use if needed\n",
    "    structured = {\n",
    "        \"top_features\": top_df,\n",
    "        \"aggregated_insights\": insights\n",
    "    }\n",
    "    return structured\n",
    "\n",
    "# Example usage:\n",
    "structured_out = generate_detailed_insights(shap_importance, shap_values, X_sample, top_n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
